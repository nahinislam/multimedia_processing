# -*- coding: utf-8 -*-
"""PCA/LDA_Nahin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROwkZcYcxhBdpJk1UmwI4TzK1VpQ-Nzc
"""

from google.colab import files


uploaded = files.upload()

import pandas as pd

# Load the Dry Beans dataset
df = pd.read_csv("Dry_Bean_Dataset.csv")  # Update the filename if needed

# Display the first few rows
df.head()

# Check for missing values
print("Missing Values:\n", df.isnull().sum())

# Display dataset info
df.info()

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# Features: All columns except the last one (target)
X = df.iloc[:, :-1].values  # 16 numerical features
y = df.iloc[:, -1].values   # Target labels (bean type)

# Encode target variable (convert text labels to numbers)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Standardize features (important for PCA & LDA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset into training & test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=1, stratify=y_encoded)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Apply PCA to reduce dimensions to 2 components (for visualization)
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_

# Plot explained variance
plt.figure(figsize=(8,5))
plt.bar(range(1, 3), explained_variance, alpha=0.7, align='center', label='Explained Variance')
plt.ylabel('Explained Variance Ratio')
plt.xlabel('Principal Components')
plt.title('PCA - Explained Variance')
plt.legend()
plt.show()

# Check new PCA-transformed data shape
X_train_pca.shape, X_test_pca.shape

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Apply LDA to reduce feature dimensions
lda = LinearDiscriminantAnalysis(n_components=2)  # Reduce to 2 linear discriminants
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)

# Plot the LDA-transformed data (first two components)
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_train_lda[:, 0], X_train_lda[:, 1], c=y_train, cmap='jet', alpha=0.7)
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('LDA Projection of Dry Beans Dataset')
plt.colorbar(scatter, label="Class Labels")
plt.show()

# Check the new LDA-transformed data shape
X_train_lda.shape, X_test_lda.shape

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Train models on original data (16 features)
logreg = LogisticRegression(max_iter=200, random_state=1)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
accuracy_logreg_original = accuracy_score(y_test, y_pred_logreg)

svm = SVC(kernel='linear', random_state=1)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)
accuracy_svm_original = accuracy_score(y_test, y_pred_svm)

# Train models on PCA-reduced data (2 features)
logreg_pca = LogisticRegression(max_iter=200, random_state=1)
logreg_pca.fit(X_train_pca, y_train)
y_pred_logreg_pca = logreg_pca.predict(X_test_pca)
accuracy_logreg_pca = accuracy_score(y_test, y_pred_logreg_pca)

svm_pca = SVC(kernel='linear', random_state=1)
svm_pca.fit(X_train_pca, y_train)
y_pred_svm_pca = svm_pca.predict(X_test_pca)
accuracy_svm_pca = accuracy_score(y_test, y_pred_svm_pca)

# Train models on LDA-reduced data (2 features)
logreg_lda = LogisticRegression(max_iter=200, random_state=1)
logreg_lda.fit(X_train_lda, y_train)
y_pred_logreg_lda = logreg_lda.predict(X_test_lda)
accuracy_logreg_lda = accuracy_score(y_test, y_pred_logreg_lda)

svm_lda = SVC(kernel='linear', random_state=1)
svm_lda.fit(X_train_lda, y_train)
y_pred_svm_lda = svm_lda.predict(X_test_lda)
accuracy_svm_lda = accuracy_score(y_test, y_pred_svm_lda)

# Display accuracy results
accuracy_results = pd.DataFrame({
    "Model": ["Logistic Regression", "SVM"],
    "Original Features (16)": [accuracy_logreg_original, accuracy_svm_original],
    "PCA Features (2)": [accuracy_logreg_pca, accuracy_svm_pca],
    "LDA Features (2)": [accuracy_logreg_lda, accuracy_svm_lda],
})

import pandas as pd
from IPython.display import display

# Assuming `accuracy_results` is a DataFrame
display(accuracy_results)  # This will show the table in a Jupyter Notebook

import matplotlib.pyplot as plt
import numpy as np

# Data for plotting
models = ["Logistic Regression", "SVM"]
original_acc = accuracy_results["Original Features (16)"].values
pca_acc = accuracy_results["PCA Features (2)"].values
lda_acc = accuracy_results["LDA Features (2)"].values

# Set figure size
plt.figure(figsize=(8, 5))

# Bar plot for accuracy comparison
x = np.arange(len(models))
width = 0.25

plt.bar(x - width, original_acc, width, label="Original Features (16)")
plt.bar(x, pca_acc, width, label="PCA Features (2)")
plt.bar(x + width, lda_acc, width, label="LDA Features (2)")

# Labels and title
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Accuracy Comparison: Original vs PCA vs LDA")
plt.xticks(x, models)
plt.ylim(0.7, 1.0)  # Set y-axis limits
plt.legend()

# Save the figure
plt.savefig("accuracy_comparison.png", dpi=300)
plt.show()

from google.colab import files

# Download the saved accuracy comparison figure
files.download("accuracy_comparison.png")